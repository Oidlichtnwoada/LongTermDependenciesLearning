% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{float}
\usepackage{amsfonts,amsthm, graphicx, trfsigns, physics}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Hannes Brantner} % The author name without titles.
\newcommand{\thesistitle}{Comparing Machine Learning Models using Long-Term Dependency and Physical System Benchmarks} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{BSc}{male}
\setadvisor{Dipl.-Ing. Dr.rer.nat.}{Radu Grosu}{BSc}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl.-Ing. Dr.}{Ramin Hasani}{BSc}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{01614466}
\setdate{31}{04}{2021} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Computer Engineering}{Technische Informatik} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

    \frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

%\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
    \addtitlepage{english} % English title page.

    \AddStatementPage

%\begin{danksagung*}
%\todo{Ihr Text hier.}
%\end{danksagung*}

    \begin{acknowledgements*}
        At first, I have to thank Ramin for providing me great support throughout my work on the thesis.
        He cared about me and was always pointing me to state-of-the-art literature, as he wanted to push me forward.
        I also have to thank Prof. Grosu for participating in numerous online meetings and for sharing his in-depth knowledge in the machine learning domain.
        Furthermore, I want to thank Mathias Lechner for giving me first-class support on questions I had regarding various machine learning models.
        I have to point out that he was always willing to help me and provided his responses incredibly fast.
        Last but not least, I have to thank my parents for providing me with mental and financial support throughout my whole study journey.
    \end{acknowledgements*}

%\begin{kurzfassung}
%\todo{Ihr Text hier.}
%\end{kurzfassung}

    \begin{abstract}
        The diversity of machine learning models has rapidly increased in recent years as research in the machine learning domain flourishes.
        This thesis tries to give an overview of machine learning models that are capable of dealing with regularly sampled time-series data without specifying a given history length that should be taken into account by the model.
        Therefore, all models presented in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
        Furthermore, new machine learning models are introduced that try to improve on the given transformer and unitary recurrent neural network \cite{EfficientUnitaryRNNs} architecture.
        After the introduction of all models, they are all benchmarked against five benchmarks and compared thoroughly.
        These benchmarks try to determine the model's capabilities to capture long-term dependencies and the ability to model physical systems.
        Moreover, a time-continuous memory cell is introduced that is capable of storing a data bit over a large number of time steps without losing the stored information.
        This memory cell is built using the LTC network \cite{LTCNetworks} architecture.
    \end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
    \selectlanguage{english}

% Add a table of contents (toc).
    \tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
    \mainmatter

    \chapter{Introduction}

    \section{Machine Learning Terms}
    A machine learning model is a mathematical parametrized function that gets an input and produces an output.
    For example, the machine learning model GPT-3 proposed in \cite{GPT-3} has 175 billion scalar parameters.
    This thesis will use imitation learning to optimally set the parameters of machine learning models.
    This means that for each input, there is an associative expected output provided that the model should return by applying its function to the input.
    Of course when the model's function is applied to the input with the initial state of the model's parameters, the returned model output will differ from the desired output in almost all cases.
    The measure that quantifies this error between model output and expected output is called a loss function and has a scalar return value.
    A sample loss function can be constructed as easy as computing the mean of all squared errors between the model output and the expected output.
    The model output is also often denoted as the prediction of the model.
    For each input sample, the loss function describes the error the model makes by applying its function and this error is only dependent on the parameters of the model.
    In the general case, a computer scientist wants to find the global minimum of that function with respect to all machine learning model parameters.
    As this is a problem that cannot be solved analytically in most cases, it is approximated by using gradient descent \cite{GradientDescent}.
    This method incrementally changes each parameter depending on the gradient of the loss function with respect to each parameter in lock step.
    By denoting the loss function with $L$, the learning rate with $\alpha$, the old whole parameter set with $p$, the old single scalar parameter with $p_i$ and the new single scalar parameter with $p_i'$, the formula to update the individual parameters $p_i$ in a single gradient descent step can be given as follows:
    \begin{align} 
        \label{gradient_descent_update}
        \forall p_i:~p_i' &= p_i - \alpha * \frac{\partial{L}}{\partial{p_i}}(p)
    \end{align}
    It is essential to note that the model as well as the loss measure must be deterministic functions for the gradient to exist.
    This update rule ensures that if the loss function increases with increasing $p_i$, therefore if the computed gradient of the loss function is larger than zero, a decrease of the parameter will happen that leads to a decreasing loss function result.
    The opposite case holds as well and this is why there is a minus sign in \ref{gradient_descent_update}.
    The learning rate $\alpha$ determines how large in magnitude the update to the parameters should be at each gradient descent step.
    A too small learning rate will lead to slow convergence, a too large learning rate will lead to divergence.
    Therefore, a too large learning rate is far more dangerous than a too small one.
    Convergence means that the parameter updates have led to a local minimum of the loss function.
    There are no guarantees that this is the global minimum. Divergence means that the loss function diverges towards infinity.
    A local minimum or convergence can be reached by applying the gradient descent update rule to as many inputs as needed to set the loss function derivative to nearly zero. 

    \section{Problem Statement}
    As the sheer amount of different machine learning models can be overwhelming, the task was to fix a distinct application domain and compare the most influential machine learning models in this domain with suitable benchmarks.
    Benchmarks are just large input data sets with associative expected outputs.
    Additionally, ideas for possible improvements in existing architectures should be implemented and benchmarked against the already existing ones.
    All benchmarked models should be implemented in the same machine learning framework and the benchmark suite should be extensible and reusable for other machine learning research projects.
    The whole implementation work done for this thesis should be made accessible for everyone by open-sourcing all the code.
    As mentioned in the abstract, all the models covered in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
    The benchmarks used in this thesis either test the models for their capabilities to capture long-term dependencies or their ability to model physical systems.
    
    \section{How to better model Physical Systems} \label{physical_systems}
    Physical systems are guided by differential equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the state derivative function $f$ and the output function $h$, both of which depend on the absolute time $t$, as follows:
    \begin{align} 
        \label{physical_system_equations_state}
        \dot x(t) &= f(x(t),u(t),t) \\
        \label{physical_system_equations_output}
        y(t) &= h(x(t),u(t),t)
    \end{align}
    This form of system description is applicable to all physical systems in our daily surroundings, most of them are even time-invariant. 
    This means the functions $f$ and $h$ do not depend on the absolute time $t$.
    For example, a mechanical pendulum will now approximately behave the same as in one year, as its dynamics do not depend on the absolute time $t$.
    The system description given in \ref{physical_system_equations_state} and \ref{physical_system_equations_output} proposes, that machine learning models that are built in a similar fashion and whose state is also determined by a differential equation, should be pretty capable of modelling the input-output relation of physical systems.
    When the benchmarked models are introduced in more detail, it can be seen that all continuous-time machine learning models use a comparable structure in terms of parameterizing the state derivative and the output function.
    
    \section{Sampled Physical Systems} \label{sampled_physical_systems}
    As the evaluation of the current state $x$ at point in time $t'$ with initial state $x_0$ given the dynamics from \ref{physical_systems} can be computationally very expensive or even infeasible, sampling was introduced to avoid solving a complex differential equation.
    Therefore, the whole system is only observed at equidistant successive time instants, values belonging to this time instant are denoted with a subscript index $k \in \mathbb{Z}$, and the system is now called discrete.
    Discrete systems are guided by difference equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the next state function $f$ and the output function $h$, both of which depend on the time instant $k$, as follows:
    \begin{align} 
        \label{discrete_system_equations_state}
        x_{k+1} &= f(x_k,u_k,k) \\
        \label{discrete_system_equations_output}
        y_k &= h(x_k,u_k,k)
    \end{align} 
    It must be noted that $x$ and $y$ are time-series in discrete systems and no more functions like in the case of continuous-time physical systems.
    This slightly off-topic explanation is necessary, as vanilla recurrent neural networks are built using the same principle. 
    The system equations \ref{discrete_system_equations_state} and \ref{discrete_system_equations_output} require a regularly (equidistantly) sampled input $x$.
    A similar argument as before in \ref{physical_systems} proposes now that a machine learning model with a similar structure, which gets a regularly sampled input of a physical system, should also be pretty capable of modelling the input-output relation of this sampled physical system.
    The corresponding machine learning models are then called discrete-time machine learning models.
    
    \section{Why capturing Long-Term Dependencies is difficult} \label{long_term_difficult}
    The difficulty will be outlined solely on the example of vanilla recurrent neural networks (RNNs).
    How transformer-based and advanced RNN architectures tackle the problem will be discussed later.
    Vanilla recurrent neural networks are discrete-time machine learning models. 
    Its dynamics are given in a similar fashion to the equations that govern sampled physical systems \ref{sampled_physical_systems}.
    The current state vector $h_{t}$ and the next input vector $x_{t+1}$ determine the next state vector $h_{t+1}$ and output vector $y_{t+1}$ deterministically.
    In this model all the past inputs are implicitly encoded in the current state vector.
    This entails a big challenge for computer scientists, as computers only allow states of finite size and finite precision, unlike our physical environment, which results in an information bottleneck in the state vector.
    The next state of a vanilla recurrent neural network $h_{t+1}$ and its output $y_{t}$ is typically computed by equations like the two proposed in \cite[p. 2]{UnitaryRNNs} by using a non-linear bias-parametrized activation function $\sigma$, three matrices ($W$, $V$ and $U$) and the output bias vector $b_o$:
    \begin{align}
        \label{rnn_state_update}
        h_{t+1} &= \sigma(W*h_t + V*x_{t+1}) \\
        \label{rnn_output}
        y_{t} &= U*h_{t} + b_o
    \end{align}
    Without the time shift on the input in the next state equation \ref{rnn_state_update}, the equations are pretty similar to the ones describing sampled physical systems.
    The following inequality from \cite[p. 2]{UnitaryRNNs} using norms shows the relation between the loss derivative, a recent state $h_T$ and a state from the distant past $h_t$ where $T \gg t$.
    The notation is kept similar to the examples before. A subscript $2$ after a vector norm denotes the Euclidean norm and a subscript $2,ind$ after a matrix norm denotes the spectral norm:
    \begin{align}
        \label{gradient_formula}
        \left\Vert \pdv{L}{h_t} \right\Vert_2 \leq \left\Vert \pdv{L}{h_T} \right\Vert_2 * \left\Vert W \right\Vert_{2,ind}^T * \prod^{T-1}_{k=t} \left\Vert diag(\sigma'(W*h_k + V*x_{k+1})) \right\Vert_{2,ind}
    \end{align}
    This inequality contains all essential parts to understand why capturing long-term dependencies with vanilla recurrent neural networks is difficult.
    Some problems that machine learning tries to solve require incorporating input data from the distant past to make good predictions in the present.
    As these inputs are implicitly encoded in the states of the distant past, $\left\Vert \pdv{L}{h_t} \right\Vert_2$ should not decay to zero or grow unboundedly to effectively tune the parameters using the gradient descent update rule shown above in \ref{gradient_descent_update}.
    This ensures that distant past inputs influence the loss function reasonably and makes it feasible to incorporate the knowledge to minimize the loss function.
    As known the spectral norm of the diagonal matrix in \ref{gradient_formula} is just the largest magnitude out of all diagonal entries.
    Therefore, if the norm of the diagonal matrix is close to zero over multiple time steps $k$, also the desired loss gradient will decay towards zero.
    Otherwise, if the norm of the diagonal matrix is much larger than one over multiple time steps $k$, the desired loss gradient may grow unboundedly.
    Using this knowledge it is now clear that a suitable activation function must have a derivative of one in almost all cases to counteract the above described problems.
    A good fit would be a rectified linear unit (relu) activation function with an added bias term.
    The relu activation function with a bias $b$ can simply be discribed by the function $max(0,x+b)$. The $max$ function should be applied element-wise.
    As the requirements for the activation function candidates are clear now, the next thing to discuss is the norm of the matrix $W$.
    If $\left\Vert W \right\Vert_{2,ind} > 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ may grow unboundedly, making it difficult to apply the gradient descent technique to optimize parameters.
    If $\left\Vert W \right\Vert_{2,ind} < 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ will decay to $0$, making it impossible to apply the gradient descent technique to optimize parameters.
    These problems are identical to the problems regarding the norm of the diagonal matrix and also have the same implications.
    The first case is calles the exploding gradient problem and the second case is called the vanishing gradient problem for given reasons.
    Both phenomena are explained in more detail in \cite{LongTermDependenciesGradientDescent}.

    \section{Aim of the Work}
    This work should help to objectively compare various machine learning models used to process regularly sampled time-series data.
    It should outline the weaknesses and strengths of the benchmarked models and determine their primary domain of use.
    Moreover, as there are many models benchmarked, their relative expressivity across various application domains can be compared reasonably well.
    Another aim is to provide an overview of what architectures are currently available and how they can be implemented.
    Furthermore, the implemented benchmark suite should be reusable for future projects in the machine learning domain.

    \section{Methodological Approach}
    The first part of this thesis was to determine the most influential models for processing time-series data.
    Some of the models that were benchmarked against each other in this thesis were taken from \cite{ODELSTM}, even though this paper focuses primarily on irregularly sampled time-series.
    The other models were implemented according to the following architectures: Long Short-Term Memory \cite{LSTM}, Differentiable Neural Computer \cite{DNC}, Unitary Recurrent Neural Network \cite{EfficientUnitaryRNNs}, Transformer \cite{Transformer} and Neural Circuit Policies \cite{NCP}.
    These nine models are then complemented by five models that were newly introduced.
    All these models are benchmarked against each other.
    Additionally, a time-continuous memory cell architecture should be introduced.
    This architecture must have its own benchmark test and should not be benchmarked against all other fully-fledged machine learning models as it is only a proof-of-concept implementation.
    All mentioned models should be implemented in the machine learning framework Tensorflow \cite{Tensorflow}.
    After the implementation of all models, an extensible benchmark suite had to be implemented to compare all implemented models.
    A basic benchmark framework should be implemented, which automatically trains a given model and saves all relevant information regarding the training process including generating plots to visualize the data.
    All that should be needed to implement a new benchmark is to specify the input, the expected output data, the loss function and the required output vector size of the model.
    The benchmarks regarding person activity classification, sequential MNIST classification and kinematic physics simulation were taken from \cite{ODELSTM} and were modified slightly to be compatible with the benchmark framework.
    The other two benchmark regarding the copying memory and the adding problem were taken from \cite{UnitaryRNNs}, but were also slightly modified to fit the benchmark framework's needs.
    The sixth benchmark that had to be implemented was the cell benchmark that should check if the memory cell is able to store information over a large number of time steps.
    When this step is also done, all benchmarks should be run on all applicable models and then the results should be thoroughly compared to filter out the strengths and weaknesses of the diverse models.
    Only after that a summary should be written to concisely summarize the most important discoveries and fallacies that were made.

    \section{State of the Art}
    The whole field of sequence modeling started with recurrent neural networks.
    More and more modern machine learning architectures exploit the fact that continuous-time models are very well suited for tasks related to dynamical physical systems as explained in \ref{physical_systems}.
    A few examples for such models would be the CT-GRU \cite{CTGRU}, the LTC network \cite{LTCNetworks} and the ODE-LSTM architecture \cite{ODELSTM}.
    But there are also some older architectures that exploit continuous-time dynamics in machine learning models like the CT-RNN architecture \cite{CTRNN}.
    The other problem described in the previous chapters is the hard task of capturing lon-term dependencies in time-series.
    One solution for the problem was proposed in \cite{UnitaryRNNs}, which introduced the Unitary RNN architecture.
    This architecture in principle just uses the vanilla RNN architecture described above, but with the difference that the matrix $W$ fulfills $\left\Vert W \right\Vert_{2,ind} = 1$ to tackle the vanishing and exploding gradient problem.
    This idea was later refined by \cite{EfficientUnitaryRNNs}.
    The vanishing gradient problem was also tackled by the LSTM architecture \cite{LSTM} using a mechanism called gating. 
    This mechanism changes the next state computation of the vanilla RNN.
    Another possible mitigation to the vanishing gradient problem is the transformer architecture proposed in \cite{Transformer} using a mechanism called attention.
    In principle the transformer architecture model has access to all past inputs at a single time-step and directs its attention to the inputs most relevant for solving the required task.
    This eliminates the need to backpropagate the error through multiple time-steps, which keeps the number of backpropagation steps low. 
    
%\todo{Enter your text here.}

    \chapter{Models}

    \section{LSTM} \label{lstm}
    The LSTM (Long Short-Term Memory) recurrent neural network architecture is a discrete-time machine learning model as introduced in \ref{sampled_physical_systems}.
    The model not only has an ordinary (hidden) state vector, but also a cell state vector which should store information over a longer time horizon than the hidden state vector.
    This thesis uses the open-source LSTM implementation provided by the Keras library \cite{Keras} which is based on the original LSTM paper \cite{LSTM} as well on its successor paper \cite{LSTM_forget} that introduces a forget mechanism for the LSTM.
    The function the LSTM model is applying to its inputs to produce the outputs is given as follows with inputs denoted as $x_t$ and outputs which equals the hidden states denoted as $h_t$ \cite[p. 6-9]{LSTM}:
    \begin{align}
    \label{forget_gate} f_t &= \sigma_s(W_f*x_t + U_f*h_{t-1} + b_f) \\
    \label{input_gate} i_t &= \sigma_s(W_i*x_t + U_i*h_{t-1} + b_i) \\
    \label{output_gate} o_t &= \sigma_s(W_o*x_t + U_o*h_{t-1} + b_o) \\
    \label{cell_input} \tilde{c}_t &= \sigma_h(W_c*x_t + U_c*h_{t-1} + b_c) \\
    \label{cell_state} c_t &= f_t * c_{t-1} + i_t * \tilde{c}_t \\
    \label{hidden_state} h_t &= o_t * \sigma_h(c_t)
    \end{align}
    The function $\sigma_s$ denotes the sigmoid function and the the function $\sigma_h$ denotes the hyperbolic tangent function.
    The term $f_t$ \ref{forget_gate} is the forget gate's activation vector, $i_t$ \ref{input_gate} is the input gate's activation vector, $o_t$ \ref{output_gate} is the output gate's activation vector, $\tilde{c}_t$ \ref{cell_input} is the cell input activation vector, $c_t$ \ref{cell_state} is the cell state vector and $h_t$ \ref{hidden_state} is the hidden state vector or also called output vector of the LSTM model.
    The initial hidden state $h_0$ and the initial cell state $c_0$ are picked to the all-zero vector.
    Matrices are denoted with capital letters and vectors are denoted with lower case letters. 
    The LSTM model has a configurable state size. The multiplication sign between two vectors denotes a scalar product and it denotes matrix multiplication between matrices and vectors.
    This convention is used throughout this thesis.
    Dimensions of matrices are picked such that the resulting vector has the required state size which is configurable.
    The bias vectors denoted with $b$ also have the required state dimension.
    The matrices denoted by $W$ map the input vector in each time step and the matrices denoted by $U$ map the hidden state vector at each time step to a resulting vector.
    The structure of the model allows it to capture long-term dependencies as by setting $f_t$ equal to one and $i_t$ equal to zero in some common vector indices $i$, only the previous cell state is used to build the next cell state in these next cell state vector entries.
    This will lead to $\frac{\partial{c_{t,i}}}{\partial{c_{t-1,i}}} = 1$, as this clearly approximates the identity function for a specific index $i$ in the cell state vector.
    Backpropagation to activations in the distant past is feasible using this model function as gradients are not vanishing or exploding when the parameters of the model are learnt properly.
    This mechanism is called the constant error carrousel as described in \cite[p. 7]{LSTM}.
    LSTMs can incorporate this mechanism to store essential information from the distant past, which may be useful to make accurate predictions in the future.
    Furthermore, the model can also decide to forget the previous cell state completely, if the current input vector makes the stored cell state obsolete in the corresponding application.
    This is done by learning to set the forget gate's activation vector close to zero and the cell input activation vector is then used to fill the cell state again if the input gate's activation vector is set accordingly.
    The output gate's activation vector determines which portion of the cell state is used to build the hidden state or output vector of the LSTM model. 
    Throughout the thesis an LSTM model with a fixed state vector size of $64$ was used. As mentioned in the benchmark framework section, each model must support an arbitrary output vector size.
    This is accomplished by postprocessing the hidden state outputs of the LSTM by a dense layer with the required amount of output neurons and without an activation function.
    The output $y$ of a dense layer without an activation function and input vector $x$ can simply be given by: $y = W*x + b$.
    In this notation $W$ is a matrix such that it maps the input vector $x$ to the required output size and $b$ is just a bias vector like in the functions describing the LSTM model.
    Training the LSTM model from the Keras library is fast as it uses an optimized cuDNN \cite{cuDNN} implementation.

    \section{GRU} \label{gru}
    The GRU (Gated Recurrent Unit) recurrent neural network architecture is a discrete-time machine learning model as introduced in \ref{sampled_physical_systems}.
    The model has only a single ordinary hidden state vector.
    This thesis uses the open-source GRU implementation provided by the Keras library \cite{Keras} which is based on the original GRU paper \cite{GRU}.
    The GRU model tries to simplify the LSTM architecture by removing the output gate for example without sacrificing expressivity.
    This leads to a smaller parameter count of a GRU model with the same hidden state vector size as an LSTM model.
    The function the GRU model is applying to its inputs to produce the outputs is given as follows with inputs denoted as $x_t$ and outputs which equals the hidden states denoted as $h_t$ \cite[p. 4]{GRU}:
    \begin{align}
    \label{update_gate} z_t &= \sigma_s(W_z*x_t + U_z*h_{t-1} + b_z) \\
    \label{reset_gate} r_t &= \sigma_s(W_r*x_t + U_r*h_{t-1} + b_r) \\
    \label{candidate_activation_vector} \tilde{h}_t &= \sigma_h(W_h*x_t + U_h*(r_t * h_{t-1}) + b_h) \\
    \label{output_vector} h_t &= (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t \\
    \end{align}
    The function $\sigma_s$ denotes the sigmoid function and the the function $\sigma_h$ denotes the hyperbolic tangent function.
    The term $z_t$ \ref{update_gate} is the update gate vector, $r_t$ \ref{reset_gate} is reset gate vector, $\tilde{h}_t$ \ref{candidate_activation_vector} is the candidate activation vector and $h_t$ is the hidden state or output vector of the GRU model.
    The initial hidden state $h_0$ is picked to the all-zero vector.
    The notation of operations, matrices and vectors stays the same as for the LSTM architecture \ref{lstm}.
    Subtraction in the output vector equation \ref{output_vector} is meant element-wise and the $1$ should denote the all-one vector.
    As in the LSTM architecture the hidden state vector size is configurable and all matrices map their inputs to a vector of the corresponding hidden state vector size.
    The structure of the model allows it to capture long-term dependencies as by setting $z_t$ equal to zero for some vector entries, only the previous hidden state vector is used to build the next hidden state vector for these indices $i$.
    This will lead to $\frac{\partial{h_{t,i}}}{\partial{h_{t-1,i}}} = 1$, as this clearly approximates the identity function for a specific index $i$ in the hidden state vector.
    Backpropagation to activations in the distant past is feasible using this model function as gradients are not vanishing or exploding when the parameters of the model are learnt properly.
    As also mentioned in \cite[p. 5]{GRU}, the LSTM architecture does not expose its full cell state in the output vector as the cell state is further processes using the output gate.
    The GRU architecture however exposes its full cell state at each time step as it does not have an output gate as mentioned before.
    Another key difference between the LSTM and GRU architecture is that the LSTM architecture controls the portions of the previous cell state and the portions of the cell input activation that add up to the next step cell state separately using the forget gate's activation vector and the input gate's activation vector \ref{cell_state}.
    The GRU model simplifies this mechanism by providing just a single update gate vector $z$.
    The other vector controlling the portion from the previous hidden state that is added together to build the next step hidden state vector is then determined by subtracting $z$ from the all-one vector \ref{output_vector}.
    This is feasible as the sigmoid activation function produces only outputs lying in the interval $[0,1]$. 
    Furthermore, also the reset mechanism works differently in the GRU architecture as the reset vector only operates on the previous step hidden state vector when computing the next state candidate activation vector.
    Throughout the thesis a GRU model with a fixed state vector size of $80$ was used. As mentioned in the benchmark framework section, each model must support an arbitrary output vector size.
    This is accomplished by postprocessing the hidden state outputs with a dense layer just like in the case of the LSTM architecture \ref{lstm}.
    Training the GRU model from the Keras library is fast as it uses an optimized cuDNN \cite{cuDNN} implementation.
 
    \section{CT-RNN}

    \section{CT-GRU}

    \section{ODE-LSTM}

    \section{Neural Circuit Policies (NCP)}

    \section{Unitary RNN}

    \section{Matrix Exponential Unitary RNN}

    \section{Unitary NCP}

    \section{Transformer}

    \section{Recurrent Network Augmented Transformer}

    \section{Recurrent Network Attention Transformer}

    \section{Memory Augmented Transformer}

    \section{Differentiable Neural Computer} \label{Differentiable Neural Computer}
    This model defines a memory-augmented neural network architecture, that consists of a controller, read and write heads and obviously an external memory that is not parameterized by the neural network parameter.
    Furthermore, the external memory was structured in rows, where each memory row has a specific length.
    The architecture was taken from \cite{DNC} and is an enhancement to the Neural Turing Machine firstly introduced in \cite{NTM}.
    The Neural Turing Machine introduced differentiable read and write functions that allow to access the memory by context or by location in both read and write mode.
    The access by context was implemented by comparing the cosine distance of an emitted key vector to all memory row contents and by applying the softmax function to that distance vector, which yields a weight vector.
    The access by location was implemented by adding a possibility to interleave the previous step weights with the current step content-based weights and adding a "blurry" shift operation on top of it.
    These weight vectors are then normalized using a softmax function that takes each argument to the power of an emitted number to sharpen the weights.
    Some improvements of the Differentiable Neural Computer include a memory management system that is able to allocate and free memory in the external memory to avoid overwriting of important information and a memory use link matrix that allows the model to track its memory operations through time.

    \section{Memory Cell} \label{Memory Cell}
    The Memory Cell is a simple RNN consisting of two LTC neurons as described in \cite{LTCNetworks} and used in \cite{NCP}.
    However, the leakage term was removed from the ordinary differential equation describing the state dynamics, as a fading potential would mean losing information stored in the memory cell over time.
    It has a 2-dimensional input, the input voltage for each of the two neurons delivered by a synapse, and a 1-dimensional output, which is just the potential of the first neuron.
    This model was implemented to tune state dynamics and parameters for the suggested memory cell architecture.
    Each cell has three incoming synapses:
    \begin{itemize}
        \item{}
        an excitatory synapse that delivers the input voltage over a synapse to the neuron
        \item{}
        a recurrent excitatory synapse that connects each neuron with itself, which is useful to maintain an excitation in a single neuron
        \item{}
        an inhibitory synapse from the other neuron, to create mutual exclusive excitation
    \end{itemize}
    This simple memory cell should now be able to store information over a long time horizon, the input vectors are provided in the right way.
    Recent results have shown that this architecture is not capable of repeatedly storing information and I am not sure in what range the input values shall lie.
    As described in \cite{LTCNetworks}, the synaptic current is computed by multiplying a non-linear conductance with a potential difference.
    The potential difference is just a parameter $E_{ij}$ minus the potential of the postsynaptic neuron $V_j$.
    However, if the potentials are not bounded, this would mean even an excitatory synapse can deliver a negative current or analogously an inhibitory synapse can deliver a positive current.
    The bounded dynamics of LTC networks is no longer valid, as the leakage term was removed from the state dynamics equation.

    \chapter{Benchmarks}

    \section{Benchmark Framework}
    \subsection{Setup}
    A single code base to run and evaluate the diverse set of benchmarks and models was inevitable. 
    Otherwise, the whole project would have been unmanageable.
    As the implementation of all models occurred in the \texttt{Python} programming language \cite{Python3} using the framework Tensorflow \cite{Tensorflow}, also the benchmark framework used the same set of tools.  
    Therefore, a benchmark base class was created in the file \texttt{benchmark.py}, which is available under the URL \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/benchmark.py}.
    The creation of a new benchmark is as easy as subclassing the benchmark base class \texttt{Benchmark}.
    For instructions how to call the newly created class, please consulate the \texttt{README.md} file given under the URL \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/README.md}.
    After subclassing the base class, the new class has to correctly call the superclass constructor and overwrite the abstract method \texttt{get\_data\_and\_output\_size}.
    Furthermore, the new benchmark's name should be added to the \texttt{BENCHMARK\_NAMES} list constant.
    The superclass constructor only has two arguments: \texttt{name} and \texttt{parser\_configs}. 
    The first argument is just the name of the new benchmark passed as a string.
    The second argument should be a tuple of individual parser configs. 
    A parser config is itself a tuple consisting of the argument name, the argument default value and the argument type.
    This argument determines which values should be settable and usable when calling the benchmark from the command line.
    There are at least three parser configs required that set the loss name, the loss config and the metric name.
    A sample \texttt{parser\_configs} argument would be: \texttt{(('-{}-loss\_name', 'SparseCategoricalCrossentropy', str), ('-{}-loss\_config', {'from\_logits': True}, dict), ('-{}-metric\_name', 'SparseCategoricalAccuracy', str))}.
    If loss config or metric name is not applicable to the benchmark, simply set the default loss config to \texttt{\{\}} or the default metric name to \texttt{''}.
    Furthermore, if the benchmark needs additional parameters, just extend the \texttt{parser\_configs} parameter to also include the desired command line arguments.
    This feature is used by all individual benchmark implementations. 
    After calling the superclass constructor all command line arguments configured through \texttt{parser\_configs} will be available by their names as properties of \texttt{self.args} without the double hyphen.
    For example the loss name can be accessed by \texttt{self.args.loss\_name}.
    If some parameters were set through the command line, they will have the corresponding value, otherwise the configured default values will be applied.
    After that the benchmark base class will create paths for some required directories. 
    There are five directories required during benchmark execution: a saved model directory (will be created to save the models together with their best weights during training), a TensorBoard directory (will be created to save TensorBoard logs for eventual later evaluation), a supplementary data directory (already present in the repo to pass input data to the benchmark), a result directory (will be created to save csv files with relevant information about the training process) and a visualization directory (will be created to save visualizations created after each training of a model).
    All these paths start in the root folder of the repository called \texttt{NeuralNetworkArena}. 
    The structure how these paths continue is the same for all five kinds of folders.
    For the next step in path creation, the individual name for the required folder kind will be appended to the root folder.
    These names can be passed as a command argument when calling the individual benchmark classes. 
    For a more in detail description of these command line parameters just call an implemented benchmark class with the \texttt{-{}-h} command line parameter as described in the \texttt{README.md} file.
    Then the name of the individual benchmarks is added to the path, such that each benchmark has its own five subfolders.
    Then the benchmark base class calls its \texttt{get\_data\_and\_output\_size} method that should have been implemented by the subclass.
    The function should return a tuple of inputs, a tuple of expected outputs and an output vector size of the machine learning model.
    The input and output tuple should only contain numpy arrays \cite{numpy}. The output tuple must have size exactly one.
    The input tuple must have size at least one.
    The benchmark base class has also support for time inputs to the models.
    Please make sure that the time input is the last entry in the input tuple.
    There is also the command line argument called \texttt{use\_time\_input}. 
    If you want to use time input, then make sure you have this argument set to true.
    Otherwise, if the input tuple has a dimension larger than one, the last entry will be discarded from the input tuple, as it is assumed to be the time input.
    The benchmark suite works currently only for benchmarks which provide time-series input data and only expect a model output after the last input data in the time-series.
    For people familiar with the Tensorflow framework \cite{Tensorflow} this is equivalent to setting \texttt{return\_sequences=False} in an RNN model.
    All input arrays in the input tuple should have the shape \texttt{(SAMPLE\_AMOUNT, SEQUENCE\_LENGTH, INPUT\_DIMENSION)}.
    Of course the input dimension can vary between different inputs. Time data should have an input dimension of one. 
    The single output array present in the output tuple should have the shape \texttt{(SAMPLE\_AMOUNT, OUTPUT\_DIMENSION)}.
    The sample amount should match between input and output data to be valid input to the benchmark framework.
    All the constraints on the shapes will be checked by the framework and then all individual samples are shuffled such that corresponding input and output data are at the same indices in their arrays.
    Then tensors are created with the same shape as the inputs in the input tuple excluding the first dimension that denotes the sample amount.
    These are required to later use the Functional API of the Tensorflow framework \cite{Tensorflow}.
    They are created by specifying a fixed batch size, which helps the machine learning framework to better optimize the computational graph for the corresponding model.
    The default batch size is set to $128$ and can be changed by a command line parameter.
    After that, the whole samples are divided into test, validation and training samples. The amount of test and validation samples can be set via command line parameters, which default to $10\%$ each. 
    It is ensured that each individual sample set is exactly divisible by the batch size, as the computational graph was optimized by only allowing inputs of a fixed batch size as described above.
    After all the setup work is done, the folder paths to the result, the saved model and the TensorBoard directory will be augmented with the model name that is currently under test and which was passed via a command line parameter.
    Then the TensorBoard directory for that model will be deleted, as each training run creates a significant amount of log files.
    After that, the TensorBoard, the result, the saved model and visualization directory will be created if they do not already exist. 
    Then it will be checked if the passed model name is present in the list constant \texttt{MODEL\_ARGUMENTS} in the file \texttt{model\_factory.py}.
    When this check is passed, the benchmark framework either loads a saved model with the corresponding model name or it creates a new one using the model output functions in the predescribed model factory depending on the command line parameter \texttt{use\_saved\_model}.
    These output functions get an output vector size and the tensor inputs and create an output tensor that contains all the information about the operations in between.
    By knowing the input and the output tensors, the Tensorflow \cite{Tensorflow} Functional API can be incorporated to create a machine learning model.
    If the model is newly created and not loaded from a saved one, the model is also compiled using a customizable optimizer, learning rate, loss, loss config and metric.
    These can be changed by command line parameters.
    The default optimizer and learning rate used throughout all benchmarks in this thesis are the Adam optimizer \cite{Adam} and a learning rate of $10^{-3}$.
    The three remaining parameters also discussed in the previous subsection must be passed such that it is conforming with the requirements of the functions \texttt{tf.keras.optimizers.get} and \texttt{tf.keras.losses.get}.
    A debug mode can also be enabled via the command line which puts the newly created model in eager execution mode making it easier to debug the model. Furthermore, the model will be called on a single batch of inputs without invoking the model's \texttt{fit} method. This happens only in debug mode.
    In any case a model ready to train should now have been constructed and all the models characteristics including input and output shape will then be printed to the command line enabling to check if all the dimensions match the expectations.
    \subsection{Training}
    After printing available information of the model to the command line a unix timestamp is retrieved from the system to keep track of the total training duration.
    Then the training is ultimately started by invoking the model's \texttt{fit} method. This method takes the training and validation sample set, the batch size, the number of epochs and a tuple of callbacks as arguments.
    The number of epochs can also be configured in the command line, but throughout the thesis it is left to the default value of $128$.
    The \texttt{fit} method calls the machine learning model function for each batch of inputs in the training sample set. After that the model is validated on the validation sample set.
    This means the loss function is computed on the validation data, which is data that the model is not trained on.
    Validating the model should help to determine how well the model will perform on actual test data, which is also data that the model is not trained on. 
    If the loss function results for training and validation data are similar, it is said that the model generalizes well.
    When the validation step is finished the training loop proceeds with the next epoch and starts the same cycle again by providing the first batch of inputs from the training sample set.
    This cycle is repeated as often as the set value of the epochs.
    The callbacks are invoked after each completed epoch. There were five callbacks added: a ModelCheckpoint callback (saves the model with the best validation loss), an EarlyStopping callback (terminates training if the validation loss has not improved for a configurable number of epochs), a TerminateOnNan callback (terminates the training when a nan loss is encountered), a ReduceLROnPlateau callback (multiplies the learning rate by a configurable factor after no improvement of the validation loss for a configurable number of epochs) and a TensorBoard callback (saves TensorBoard log data for eventual later inspection).
    The default number of epochs used in this thesis for the EarlyStopping callback is $5$.
    Another important callback is the TerminateOnNan callback, which terminates the training loop if the loss evaluates to nan.
    This can for example happen when the loss function diverges towards infinity, therefore if the exploding gradient problem appears. 
    It may also be the case that there is a division through zero somewhere in the computational graph, which may also lead to a nan loss.
    The term nan just stands for \texttt{not a number}.
    As all benchmarked models are trained until convergence in this thesis, the ReduceLROnPlateau callback is especially important.
    The corresponding default parameters are a learning rate factor of $10^{-1}$ and a default number of epochs equal to $2$, both of which are used throughout all benchmark invocations.
    The EarlyStopping and the ReduceLROnPlateau do not see an improvement if the absolute change in the validation loss is less than $0.0001$.
    This minimum delta can also be configured via the command line, but this thesis uses the default value throughout all benchmarks.
    Furthermore, all these parameters are configurable by passing alternative values in the command line.
    After the training loop has terminated, another unix timestamp is taken to compute the total training duration.
    \subsection{Evaluation}
    The model is then evaluated using the parameters that led to the smallest validation loss during the whole training loop.
    Evaluation means that the model function is applied to the test sample set inputs and the resulting loss function result on that inputs is saved.
    The created model also provides an \texttt{evaluate} function, which takes the test sample set a batch size and another callback tuple as arguments.
    The only callback passed in the tuple is the TensorBoard callback already used in the \texttt{fit} method invocation.
    \subsection{Data Processing}
    The return values of the \texttt{fit} and \texttt{evaluate} method invocations now contain information about the means of the loss function results and of the metric function results on training, validation and test sample set.
    The means for the training and validation sample set are available for each training epoch together with the currently applied learning rate.
    All of that information is automatically accumulated in a single csv file per model for the training and the testing process. 
    The testing results of all models are also merged in a single csv containing all model results for a single benchmark.
    Data that was generated during training is automatically visualized by the benchmark base class and will be presented in a future chapter that discusses the benchmark results in more detail.
    Of course all generated files will be stored in their respective directories.

    \section{Activity Benchmark} \label{activity}
    As described in the benchmark base class, all benchmarks feature time-series data where the model output is only used after the last time step to compute the loss function.
    This benchmark uses a slightly modified person activity recognition dataset from the UCI repository \cite{UCI}.
    The mentioned dataset was distributed under the \url{https://archive.ics.uci.edu/ml/machine-learning-databases/00196/ConfLongDemo_JSI.txt}.
    The target function to learn is to map a sequence of measurements from four inertial sensors worn on the person's arms and feet to an activity classification.
    This benchmark should test a model's capability to model dynamical physical systems and understand what motion patterns belong to which class.
    The ability to capture long-term dependencies is not tested with this benchmark as the most recent input vectors should be enough to make good predictions.
    At each time step only the measurement of a single inertial sensor is presented as input to the model.
    The model can differ between the individual sensors as the modified dataset of person activity has a one-hot encoding to mark the sensor from which the current measurement is coming.
    All benchmarks feature an additional time input, where the time interval since the last input is passed on to the model if the feature is activated.
    However, this thesis has not used an additional time input for any benchmark.
    All the measurements used for this dataset were stored in the file \texttt{activity.csv} located in the supplementary data folder described in the benchmark framework section.
    The dataset is annotated with an activity classification for each time step, this benchmark however only requires the model to predict the classification corresponding to the last measurement data received.
    As the benchmark is a classification task a categorical cross-entropy loss was used that was computed from the output logits of the model.
    A categorical accuracy metric is used in this benchmark better judge how accurate the model predicts the activity class annotation corresponding to the last measurement input.
    Each model had an output vector size of seven, as there were seven different activity classes with their respective indices in brackets: lying ($0$), sitting on a chair ($1$), standing up ($2$), walking ($3$), falling ($4$), on all fours ($5$) and sitting on the ground ($6$).
    The processing of the UCI dataset was similarly done as in \cite{ODELSTM}.
    The benchmark had a configurable sequence length, maximum sample amount and sample distance.
    For this thesis, a sequence length of $64$, a maximum sample amount of $40000$ and a sample distance of $4$ was used.
    This means that each model gets a history of $64$ measurements before it has to predict the activity corresponding to the last measurement.
    The maximum sample amount should bound the number of samples and in the case of $40000$ and a sample distance of $4$, there were enough entries in the dataset file, so the benchmark was run with $40000$ samples in total.
    The sample distance is the indices offset in the dataset file between two drawn sample sequences.
    A model will get a sequence of $64$ input vectors of size seven that look like: $[0,0,0,1,4.3,1.8,0.9]$.
    The first four entries in that vector represent the one-hot encoding that describes from which one of the four sensors the measurement data was taken.
    The remaining three entries contain the x, y and z coordinate of the corresponding sensor.
    The required output vector has just one entry as it is just the index of the corresponding activity class with the mapping as described above.
    As this is a sparse class encoding, the framework has to extend this output value to a one-hot encoding to apply a cross-entropy loss between the extended one-hot encoding and the output vector of our model after a softmax function was applied.
    The softmax function is necessary to convert the so called output logits to an output probability for each class.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/activity_benchmark.py}.
    
    \section{Add Benchmark} \label{add}
    This benchmark uses a the same structure as the add benchmark introduced used in \cite{UnitaryRNNs}.
    Data for this benchmark is generated randomly at each instantiation of the benchmark.
    The target function to learn is to simply add two numbers that are marked in a much longer stream of numbers.
    At each time step a number together with a marker bit is presented as input vector to the model.
    As in the Activity Benchmark \ref{activity}, the sequence length and the sample amount are also configurable.
    For all models a sequence length of $100$ and a sample amount of $100000$ was used.
    As described above, the input vector has size two.
    The second entry is set to one only in one input vector of the first and last $50$ input vectors.
    Their distribution is uniformly across the whole first and second half of the time-series.
    In all other input vectors this second entry is set to zero. 
    The first entry of all input vectors is filled with random numbers taken independently and uniformly from the interval $[0,1)$.
    A single input vector out of the $100$ input vector each model gets during the benchmark looks like: $[0.5,1]$.
    In this example the random number is $0.5$ and it is marked, as the second entry is one.
    As described there are only two marked numbers and the expected output vector has size one and is simply the addition of both marked numbers.
    This benchmark simply uses the mean squared error loss function, as the smaller the mean square error is, the more similar the expected and the model output will be.
    Furthermore, there is no metric used in this benchmark.
    As this benchmark uses an increased sequence length of $100$ and as described the error signal is only provided after the last input vector, the model will be only able to learn this function when it is able to capture long-term dependencies.
    This means the model function must be designed in a way such that the gradient does not vanish or explode during backpropagation through the model's function.
    These problems was discussed in detail in chapter \ref{long_term_difficult}.
    When the model is not able to capture these long-term dependencies, therefore it is not able to store seen marked values in its state, the model will be forced to learn the naive memory-less strategy of always predicting one.
    This is the case as the expectation of each individual number out of the two marked ones is clearly $0.5$, as they were drawn uniformly from the given interval.
    An addition of both expectation values reveals the output of the memory-less strategy.
    As also pointed out in \cite[p. 6]{UnitaryRNNs}, this naive strategy will lead to a mean squared error of $\frac{1}{6}$.
    This can be verified as the mean squared error when constantly predicting the mean is equal to the variance of the distribution.
    As both random numbers were picked independently of each other, the variance of the distribution of the sum of both random numbers is just the sum of their individual variances.
    The distribution from which the random numbers are drawn has variance $\frac{1}{12}$.
    Therefore, adding this value to itself proves the mean square error of the memory-less strategy.
    For this benchmark, the model output vector size is simply one, as it should just contain the sum of both marked numbers.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/add_benchmark.py}.

    \section{Walker Benchmark} \label{walker}
    This benchmark evaluates how well a model can predict a dynamical, physical system's behavior and was taken from \cite{ODELSTM}.
    The training data is acquired simulation data of the \texttt{Walker2d-v2} OpenAI gym \cite{OpenAIGym} controlled by a pre-trained policy.
    The objective was to learn the kinematic simulation of the MuJoCo physics engine \cite{MuJoCo} in an auto-regressive fashion using imitation learning.
    To increase the task difficulty, the simulation data was acquired from different training stages of the pre-trained policy (between 500 and 1200 Proximal Policy Optimization iterations) and 1\% of actions were overwritten by random actions.
    Furthermore, the benchmark implements eventual frame-skips that would create an irregularly sampled time-series. 
    This feature was not used in this thesis as it covers only regularly sampled time-series.
    Only if the model understands the dynamics that are guided by differential equations, it will be able to produce accurate predictions.
    The ability to capture long-term dependencies is not tested with this benchmark as the most recent input vectors should be enough to make good predictions.
    The benchmark had a configurable sequence length, a maximum sample amount and a sample distance just like the Activity Benchmark \ref{activity}.
    Throughout the thesis a sample length of $64$, a maximum sample amount of $50000$ and a sample distance of $4$ was used.
    All parameters have the same meaning as before.
    As there were enough training data provided in \texttt{.npy} files by the creators of \cite{ODELSTM}, the benchmark had $50000$ different samples available that are partitioned in training, validation and test samples.
    The acquired simulation data can be downloaded from \url{https://pub.ist.ac.at/~mlechner/datasets/walker.zip}.
    The input sequence consists of input vectors of size $17$, which contains the current state of the physics engine at this specific time step.
    These values represent the angles of the joints and the absolute position of the bipedal robot.
    The function to learn for this benchmark is to predict the physics engine's state in the next time step by giving the machine learning model a history of the past $64$ physic engine's states.
    Therefore, the model output vector size was set to $17$ and the expected output data were also vectors of size $17$.
    As both vectors have the same size and the more similar they are, the better the prediction is, a mean squared error loss was used.
    There was no metric used for this benchmark.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/walker_benchmark.py}.

    \section{Memory Benchmark} \label{memory}
    This benchmark evaluates how well a model can capture long-term dependencies by letting the model recall past seen categories exactly.
    It is a slightly changed version of the copying memory problem described in \cite{UnitaryRNNs}.
    Input data of the benchmark input is randomly created at each invocation of the benchmark.
    There is a configurable memory length to test for, a length of the sequence to memorize, an amount of categories and an amount of samples that are randomly generated.
    Throughout the thesis a memory length of $100$, a sequence length of $1$, a category amount of $10$ and a sample amount of $100000$ were used.
    Each single input vector sequence is created by concatenating three sub sequences.
    The first sequence is the sequence to memorize of length $1$.
    It contains category indices sampled uniformly from $0$ to $9$.
    The second sequence is then just a sequence of the filler symbol $10$ repeated $100$ times.
    The third sequence is just the index of the category in the sequence to memorize that the model should recall, which is also sampled uniformly from all available indices in the sequence.
    This sequence is obviously of length $1$ and always filled with $0$ in the case of the predescribed setup.
    In total this makes up for a total sequence length of $102$ and a vector size of $1$ per time step. 
    The expected output category is encoded sparsely as in the Activity Benchmark \ref{activity} and contains a category index from $0$ to $9$ that matches the category at the index the model got at the last time step in the sequence to memorize.
    The output vector size of the model is $10$ and each output logit represents a  single category.
    As this is a classification problem, a categorical cross-entropy loss was used between the output logits of the model passed through a softmax function and the one-hot encoding extension of the sparsely encoded expected category index.
    To better visualize how good a model can actually recall the category, a categorical accuracy metric was added to this benchmark.
    It must be pointed out that a model is only capable of recalling the category seen in the first input vector if the gradient does not vanish or explode, as the error signal is only provided after the last time step.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/memory_benchmark.py}.

    \section{MNIST Benchmark} \label{mnist}
    This benchmark evaluates how well a model can capture long-term dependencies, as for correct classification the model needs to incorporate input vectors from the distant past, which will be described below.
    The idea to incorporate this benchmark was taken from \cite{ODELSTM}, which also features an event-based sequential MNIST classification problem.
    Input sequences for this benchmark were constructed from the MNIST dataset of the Keras framework \cite{Keras}.
    The MNIST dataset contains images of hand-drawn digits of size $28$ by $28$ pixels where each pixel is encoded by a single integer from $0$ to $255$.
    All images are in grey-scale and a higher integer represents a darker pixel.
    The images were vectorized to a vector of size $784$ and then split up to a sequence of vector chunks of size $8$, which results in an input sequence length of $98$.
    The expected output class index is just the digit the current image is representing.
    Furthermore, the benchmark has a configurable maximum amount of samples, which was set to $70000$.
    As the MNIST dataset had enough image samples, all specified $70000$ samples were used.
    A long-term memory of seen input chunks is indeed necessary to produce an accurate category prediction, as digits like $1$, $4$ and $9$ may be indistinguishable when only considering the most recent seen input chunks.
    This corresponds to classifying the image only based on a lower fraction of the image visible to the model, where the upper fraction was cut away.
    A model that yields accurate results must not suffer from the vanishing or exploding gradient problem, as only then the whole picture can be taken into account for classification. 
    The model output vector size was set to $10$, as each output logit should represent a single digit.
    As the expected output digit is encoded sparsely, the same procedure as in the Memory Benchmark \ref{memory} is applied to compute the categorical cross-entropy loss.
    The performance of the models was also measured by using a categorical accuracy metric, which produces a more human-interpretable result than the chosen loss function.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/mnist_benchmark.py}.

    \section{Cell Benchmark} \label{cell}
    This benchmark evaluates if the newly introduced memory cell architecture is able to store a single bit of information repeatedly including switching the memory state.
    Furthermore, it should be checked if the memory state vanishes or if it successfully persists over a long time horizon.
    This requires the ability to capture long-term dependencies as the input is provided sparsely to the model as described below.
    The benchmark has a configurable memory high symbol, memory low symbol, memory length, amount of cell switches and amount of samples that are generated at each invocation of the benchmark.
    The memory high and low symbol represent the expected output symbol when either memory state is active but the memory high symbol is also used as input symbol to sparsely activate a specific memory state, all other inputs are then set to the memory low symbol.
    The memory high symbol was picked to $1$, the memory low symbol was picked to $0$, the memory length was picked to $128$, the amount of cell switches was set to $2$ and the sample amount was set to $10000$.
    As the memory cell is a bistable memory element, there are two memory states that can be activated sparsely.
    The input vector at each time step has size $2$. If both entries are $0$, the current memory state should be kept.
    Otherwise, if a single entry is $1$ and the other entry is $0$, the corresponding memory state should be activated.
    The first part of the input sequence is constructed by activating any of the memory states sparsely as described above and the succeeding $127$ vectors are all-zero vectors.
    This sub sequence now has length $128$. The next sequence is built like the first one, but it activates the opposite cell state at the first time step, which corresponds to a cell switch.
    There are $2$ further sub sequences of this kind. The final input sequence is then the concatenation of all three sub sequences and has length $384$.
    In half of the samples either memory state is activated first in the concatenated sequence.
    The required model output vector is also given as a sequence of vectors of size $2$, therefore the error signal is provided at each time step.
    The output sequence can be easily built from the input sequence by continuing to set its entry to $1$ at the corresponding index until a new sparsely input is provided to the model.
    Therefore, the model may get the input sequence consisting of the following vectors: $[1,0],[0,0],[0,0],...,[0,1],[0,0],[0,0]$ and is required to produce the following vectors of the expected output sequence: $[1,0],[1,0],[1,0],...,[0,1],[0,1],[0,1]$.
    The sparse activation of the memory cell should lead to a permanent storage of the activation, until a new sparse input is provided to the model.
    As described above the model output vector size is $2$ and a mean squared error loss without a metric was used, as more similar vectors lead to a better prediction.
    
    \chapter{Results}

    \chapter{Summary}
%\todo{Enter your text here.}

% Remove following line for the final thesis.
% \input{intro.tex} % A short introduction to LaTeX.

    \backmatter

% Use an optional list of figures.
    \listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
    \cleardoublepage % Start list of tables on the next empty right hand page.
    \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of algorithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
    \printindex

% Add a glossary.
    \printglossaries

% Add a bibliography.
    \bibliographystyle{alpha}
    \bibliography{thesis}

\end{document}