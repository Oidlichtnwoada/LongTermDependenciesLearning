% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{float}
\usepackage{amsfonts,amsthm, graphicx, trfsigns, physics}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Hannes Brantner} % The author name without titles.
\newcommand{\thesistitle}{Comparing Machine Learning Models using Long-Term Dependency and Physical System Benchmarks} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{BSc}{male}
\setadvisor{Dipl.-Ing. Dr.rer.nat.}{Radu Grosu}{BSc}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl.-Ing. Dr.}{Ramin Hasani}{BSc}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{01614466}
\setdate{31}{04}{2021} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Computer Engineering}{Technische Informatik} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

    \frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

%\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
    \addtitlepage{english} % English title page.

    \AddStatementPage

%\begin{danksagung*}
%\todo{Ihr Text hier.}
%\end{danksagung*}

    \begin{acknowledgements*}
        At first, I have to thank Ramin for providing me great support throughout my work on the thesis.
        He cared about me and was always pointing me to state-of-the-art literature, as he wanted to push me forward.
        I also have to thank Prof. Grosu for participating in numerous online meetings and for sharing his in-depth knowledge in the machine learning domain.
        Furthermore, I want to thank Mathias Lechner for giving me first-class support on questions I had regarding various machine learning models.
        I have to point out that he was always willing to help me and provided his responses incredibly fast.
        Last but not least, I have to thank my parents for providing me with mental and financial support throughout my whole study journey.
    \end{acknowledgements*}

%\begin{kurzfassung}
%\todo{Ihr Text hier.}
%\end{kurzfassung}

    \begin{abstract}
        The diversity of machine learning models has rapidly increased in recent years as research in the machine learning domain flourishes.
        This thesis tries to give an overview of machine learning models that are capable of dealing with regularly sampled time-series data without specifying a given history length that should be taken into account by the model.
        Therefore, all models presented in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
        Furthermore, new machine learning models are introduced that try to improve on the given transformer and unitary recurrent neural network \cite{EfficientUnitaryRNNs} architecture.
        After the introduction of all models, they are all benchmarked against five benchmarks and compared thoroughly.
        These benchmarks try to determine the model's capabilities to capture long-term dependencies and the ability to model physical systems.
        Moreover, a time-continuous memory cell is introduced that is capable of storing a data bit over a large number of time steps without losing the stored information.
        This memory cell is built using the LTC network \cite{LTCNetworks} architecture.
    \end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
    \selectlanguage{english}

% Add a table of contents (toc).
    \tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
    \mainmatter

    \chapter{Introduction}

    \section{Machine Learning Terms}
    A machine learning model is a mathematical parametrized function that gets an input and produces an output.
    For example, the machine learning model GPT-3 proposed in \cite{GPT-3} has 175 billion scalar parameters.
    This thesis will use imitation learning to optimally set the parameters of machine learning models.
    This means that for each input, there is an associative expected output provided that the model should return by applying its function to the input.
    Of course when the model's function is applied to the input with the initial state of the model's parameters, the returned model output will differ from the desired output in almost all cases.
    The measure that quantifies this error between model output and expected output is called a loss function and has a scalar return value.
    A sample loss function can be constructed as easy as computing the mean of all squared errors between the model output and the expected output.
    The model output is also often denoted as the prediction of the model.
    For each input sample, the loss function describes the error the model makes by applying its function and this error is only dependent on the parameters of the model.
    In the general case, a computer scientist wants to find the global minimum of that function with respect to all machine learning model parameters.
    As this is a problem that cannot be solved analytically in most cases, it is approximated by using gradient descent \cite{GradientDescent}.
    This method incrementally changes each parameter depending on the gradient of the loss function with respect to each parameter in lock step.
    By denoting the loss function with $L$, the learning rate with $\alpha$, the old whole parameter set with $p$, the old single scalar parameter with $p_i$ and the new single scalar parameter with $p_i'$, the formula to update the individual parameters $p_i$ in a single gradient descent step can be given as follows:
    \begin{align} 
        \label{gradient_descent_update}
        \forall p_i:~p_i' &= p_i - \alpha * \frac{\partial{L}}{\partial{p_i}}(p)
    \end{align}
    It is essential to note that the model as well as the loss measure must be deterministic functions for the gradient to exist.
    This update rule ensures that if the loss function increases with increasing $p_i$, therefore if the computed gradient of the loss function is larger than zero, a decrease of the parameter will happen that leads to a decreasing loss function result.
    The opposite case holds as well and this is why there is a minus sign in \ref{gradient_descent_update}.
    The learning rate $\alpha$ determines how large in magnitude the update to the parameters should be at each gradient descent step.
    A too small learning rate will lead to slow convergence, a too large learning rate will lead to divergence.
    Therefore, a too large learning rate is far more dangerous than a too small one.
    Convergence means that the parameter updates have led to a local minimum of the loss function.
    There are no guarantees that this is the global minimum. Divergence means that the loss function diverges towards infinity.
    A local minimum or convergence can be reached by applying the gradient descent update rule to as many inputs as needed to set the loss function derivative to nearly zero. 

    \section{Problem Statement}
    As the sheer amount of different machine learning models can be overwhelming, the task was to fix a distinct application domain and compare the most influential machine learning models in this domain with suitable benchmarks.
    Benchmarks are just large input data sets with associative expected outputs.
    Additionally, ideas for possible improvements in existing architectures should be implemented and benchmarked against the already existing ones.
    All benchmarked models should be implemented in the same machine learning framework and the benchmark suite should be extensible and reusable for other machine learning research projects.
    The whole implementation work done for this thesis should be made accessible for everyone by open-sourcing all the code.
    As mentioned in the abstract, all the models covered in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
    The benchmarks used in this thesis either test the models for their capabilities to capture long-term dependencies or their ability to model physical systems.
    
    \section{How to better model Physical Systems} \label{physical_systems}
    Physical systems are guided by differential equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the state derivative function $f$ and the output function $h$, both of which depend on the absolute time $t$, as follows:
    \begin{align} 
        \label{physical_system_equations_state}
        \dot x(t) &= f(x(t),u(t),t) \\
        \label{physical_system_equations_output}
        y(t) &= h(x(t),u(t),t)
    \end{align}
    This form of system description is applicable to all physical systems in our daily surroundings, most of them are even time-invariant. 
    This means the functions $f$ and $h$ do not depend on the absolute time $t$.
    For example, a mechanical pendulum will now approximately behave the same as in one year, as its dynamics do not depend on the absolute time $t$.
    The system description given in \ref{physical_system_equations_state} and \ref{physical_system_equations_output} proposes, that machine learning models that are built in a similar fashion and whose state is also determined by a differential equation, should be pretty capable of modelling the input-output relation of physical systems.
    When the benchmarked models are introduced in more detail, it can be seen that all continuous-time machine learning models use a comparable structure in terms of parameterizing the state derivative and the output function.
    
    \section{Sampled Physical Systems} \label{sampled_physical_systems}
    As the evaluation of the current state $x$ at point in time $t'$ with initial state $x_0$ given the dynamics from \ref{physical_systems} can be computationally very expensive or even infeasible, sampling was introduced to avoid solving a complex differential equation.
    Therefore, the whole system is only observed at equidistant successive time instants, values belonging to this time instant are denoted with a subscript index $k \in \mathbb{Z}$, and the system is now called discrete.
    Discrete systems are guided by difference equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the next state function $f$ and the output function $h$, both of which depend on the time instant $k$, as follows:
    \begin{align} 
        \label{discrete_system_equations_state}
        x_{k+1} &= f(x_k,u_k,k) \\
        \label{discrete_system_equations_output}
        y_k &= h(x_k,u_k,k)
    \end{align} 
    It must be noted that $x$ and $y$ are time series in discrete systems and no more functions like in the case of continuous-time physical systems.
    This slightly off-topic explanation is necessary, as vanilla recurrent neural networks are built using the same principle. 
    The system equations \ref{discrete_system_equations_state} and \ref{discrete_system_equations_output} require a regularly (equidistantly) sampled input $x$.
    A similar argument as before in \ref{physical_systems} proposes now that a machine learning model with a similar structure, which gets a regularly sampled input of a physical system, should also be pretty capable of modelling the input-output relation of this sampled physical system.
    The corresponding machine learning models are then called discrete-time machine learning models.
    
    \section{Why capturing Long-Term Dependencies is difficult}
    The difficulty will be outlined solely on the example of vanilla recurrent neural networks (RNNs).
    How transformer-based and advanced RNN architectures tackle the problem will be discussed later.
    Vanilla recurrent neural networks are discrete-time machine learning models. 
    Its dynamics are given in a similar fashion to the equations that govern sampled physical systems \ref{sampled_physical_systems}.
    The current state vector $h_{t}$ and the next input vector $x_{t+1}$ determine the next state vector $h_{t+1}$ and output vector $y_{t+1}$ deterministically.
    In this model all the past inputs are implicitly encoded in the current state vector.
    This entails a big challenge for computer scientists, as computers only allow states of finite size and finite precision, unlike our physical environment, which results in an information bottleneck in the state vector.
    The next state of a vanilla recurrent neural network $h_{t+1}$ and its output $y_{t}$ is typically computed by equations like the two proposed in \cite[p. 2]{UnitaryRNNs} by using a non-linear bias-parametrized activation function $\sigma$, three matrices ($W$, $V$ and $U$) and the output bias vector $b_o$:
    \begin{align}
        \label{rnn_state_update}
        h_{t+1} &= \sigma(W*h_t + V*x_{t+1}) \\
        \label{rnn_output}
        y_{t} &= U*h_{t} + b_o
    \end{align}
    Without the time shift on the input in the next state equation \ref{rnn_state_update}, the equations are pretty similar to the ones describing sampled physical systems.
    The following inequality from \cite[p. 2]{UnitaryRNNs} using norms shows the relation between the loss derivative, a recent state $h_T$ and a state from the distant past $h_t$ where $T \gg t$.
    The notation is kept similar to the examples before. A subscript $2$ after a vector norm denotes the Euclidean norm and a subscript $2,ind$ after a matrix norm denotes the spectral norm:
    \begin{align}
        \label{gradient_formula}
        \left\Vert \pdv{L}{h_t} \right\Vert_2 \leq \left\Vert \pdv{L}{h_T} \right\Vert_2 * \left\Vert W \right\Vert_{2,ind}^T * \prod^{T-1}_{k=t} \left\Vert diag(\sigma'(W*h_k + V*x_{k+1})) \right\Vert_{2,ind}
    \end{align}
    This inequality contains all essential parts to understand why capturing long-term dependencies with vanilla recurrent neural networks is difficult.
    Some problems that machine learning tries to solve require incorporating input data from the distant past to make good predictions in the present.
    As these inputs are implicitly encoded in the states of the distant past, $\left\Vert \pdv{L}{h_t} \right\Vert_2$ should not decay to zero or grow unboundedly to effectively tune the parameters using the gradient descent update rule shown above in \ref{gradient_descent_update}.
    This ensures that distant past inputs influence the loss function reasonably and makes it feasible to incorporate the knowledge to minimize the loss function.
    As known the spectral norm of the diagonal matrix in \ref{gradient_formula} is just the largest magnitude out of all diagonal entries.
    Therefore, if the norm of the diagonal matrix is close to zero over multiple time steps $k$, also the desired loss gradient will decay towards zero.
    Otherwise, if the norm of the diagonal matrix is much larger than one over multiple time steps $k$, the desired loss gradient may grow unboundedly.
    Using this knowledge it is now clear that a suitable activation function must have a derivative of one in almost all cases to counteract the above described problems.
    A good fit would be a rectified linear unit (relu) activation function with an added bias term.
    The relu activation function with a bias $b$ can simply be discribed by the function $max(0,x+b)$. The $max$ function should be applied element-wise.
    As the requirements for the activation function candidates are clear now, the next thing to discuss is the norm of the matrix $W$.
    If $\left\Vert W \right\Vert_{2,ind} > 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ may grow unboundedly, making it difficult to apply the gradient descent technique to optimize parameters.
    If $\left\Vert W \right\Vert_{2,ind} < 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ will decay to $0$, making it impossible to apply the gradient descent technique to optimize parameters.
    These problems are identical to the problems regarding the norm of the diagonal matrix and also have the same implications.
    The first case is calles the exploding gradient problem and the second case is called the vanishing gradient problem for given reasons.
    Both phenomena are explained in more detail in \cite{LongTermDependenciesGradientDescent}.

    \section{Aim of the Work}
    This work should help to objectively compare various machine learning models used to process regularly sampled time-series data.
    It should outline the weaknesses and strengths of the benchmarked models and determine their primary domain of use.
    Moreover, as there are many models benchmarked, their relative expressivity across various application domains can be compared reasonably well.
    Another aim is to provide an overview of what architectures are currently available and how they can be implemented.
    Furthermore, the implemented benchmark suite should be reusable for future projects in the machine learning domain.

    \section{Methodological Approach}
    The first part of this thesis was to determine the most influential models for processing time-series data.
    Some of the models that were benchmarked against each other in this thesis were taken from \cite{ODELSTM}, even though this paper focuses primarily on irregularly sampled time-series.
    The other models were implemented according to the following architectures: Long Short-Term Memory \cite{LSTM}, Differentiable Neural Computer \cite{DNC}, Unitary Recurrent Neural Network \cite{EfficientUnitaryRNNs}, Transformer \cite{Transformer} and Neural Circuit Policies \cite{NCP}.
    These nine models are then complemented by five models that were newly introduced.
    All these models are benchmarked against each other.
    Additionally, a time-continuous memory cell architecture should be introduced.
    This architecture must have its own benchmark test and should not be benchmarked against all other fully-fledged machine learning models as it is only a proof-of-concept implementation.
    All mentioned models should be implemented in the machine learning framework Tensorflow \cite{Tensorflow}.
    After the implementation of all models, an extensible benchmark suite had to be implemented to compare all implemented models.
    A basic benchmark framework should be implemented, which automatically trains a given model and saves all relevant information regarding the training process including generating plots to visualize the data.
    All that should be needed to implement a new benchmark is to specify the input, the expected output data, the loss function and the required output vector size of the model.
    The benchmarks regarding person activity classification, sequential MNIST classification and kinematic physics simulation were taken from \cite{ODELSTM} and were modified slightly to be compatible with the benchmark framework.
    The other two benchmark regarding the copying memory and the adding problem were taken from \cite{UnitaryRNNs}, but were also slightly modified to fit the benchmark framework's needs.
    The sixth benchmark that had to be implemented was the cell benchmark that should check if the memory cell is able to store information over a large number of time steps.
    When this step is also done, all benchmarks should be run on all applicable models and then the results should be thoroughly compared to filter out the strengths and weaknesses of the diverse models.
    Only after that a summary should be written to concisely summarize the most important discoveries and fallacies that were made.

    \section{State of the Art}
    The whole field of sequence modeling started with recurrent neural networks.
    More and more modern machine learning architectures exploit the fact that continuous-time models are very well suited for tasks related to dynamical physical systems as explained in \ref{physical_systems}.
    A few examples for such models would be the CT-GRU \cite{CTGRU}, the LTC network \cite{LTCNetworks} and the ODE-LSTM architecture \cite{ODELSTM}.
    But there are also some older architectures that exploit continuous-time dynamics in machine learning models like the CT-RNN architecture \cite{CTRNN}.
    The other problem described in the previous chapters is the hard task of capturing lon-term dependencies in time-series.
    One solution for the problem was proposed in \cite{UnitaryRNNs}, which introduced the Unitary RNN architecture.
    This architecture in principle just uses the vanilla RNN architecture described above, but with the difference that the matrix $W$ fulfills $\left\Vert W \right\Vert_{2,ind} = 1$ to tackle the vanishing and exploding gradient problem.
    This idea was later refined by \cite{EfficientUnitaryRNNs}.
    The vanishing gradient problem was also tackled by the LSTM architecture \cite{LSTM} using a mechanism called gating. 
    This mechanism changes the next state computation of the vanilla RNN.
    Another possible mitigation to the vanishing gradient problem is the transformer architecture proposed in \cite{Transformer} using a mechanism called attention.
    In principle the transformer architecture model has access to all past inputs at a single time-step and directs its attention to the inputs most relevant for solving the required task.
    This eliminates the need to backpropagate the error through multiple time-steps, which keeps the number of backpropagation steps low. 
    
%\todo{Enter your text here.}

    \chapter{Models}

    \section{LSTM}

    \section{GRU}

    \section{CT-RNN}

    \section{CT-GRU}

    \section{ODE-LSTM}

    \section{Neural Circuit Policies (NCP)}

    \section{Unitary RNN}

    \section{Matrix Exponential Unitary RNN}

    \section{Unitary NCP}

    \section{Transformer}

    \section{Recurrent Network Augmented Transformer}

    \section{Recurrent Network Attention Transformer}

    \section{Memory Augmented Transformer}

    \section{Differentiable Neural Computer} \label{Differentiable Neural Computer}
    This model defines a memory-augmented neural network architecture, that consists of a controller, read and write heads and obviously an external memory that is not parameterized by the neural network parameter.
    Furthermore, the external memory was structured in rows, where each memory row has a specific length.
    The architecture was taken from \cite{DNC} and is an enhancement to the Neural Turing Machine firstly introduced in \cite{NTM}.
    The Neural Turing Machine introduced differentiable read and write functions that allow to access the memory by context or by location in both read and write mode.
    The access by context was implemented by comparing the cosine distance of an emitted key vector to all memory row contents and by applying the softmax function to that distance vector, which yields a weight vector.
    The access by location was implemented by adding a possibility to interleave the previous step weights with the current step content-based weights and adding a "blurry" shift operation on top of it.
    These weight vectors are then normalized using a softmax function that takes each argument to the power of an emitted number to sharpen the weights.
    Some improvements of the Differentiable Neural Computer include a memory management system that is able to allocate and free memory in the external memory to avoid overwriting of important information and a memory use link matrix that allows the model to track its memory operations through time.

    \section{Memory Cell} \label{Memory Cell}
    The Memory Cell is a simple RNN consisting of two LTC neurons as described in \cite{LTCNetworks} and used in \cite{NCP}.
    However, the leakage term was removed from the ordinary differential equation describing the state dynamics, as a fading potential would mean losing information stored in the memory cell over time.
    It has a 2-dimensional input, the input voltage for each of the two neurons delivered by a synapse, and a 1-dimensional output, which is just the potential of the first neuron.
    This model was implemented to tune state dynamics and parameters for the suggested memory cell architecture.
    Each cell has three incoming synapses:
    \begin{itemize}
        \item{}
        an excitatory synapse that delivers the input voltage over a synapse to the neuron
        \item{}
        a recurrent excitatory synapse that connects each neuron with itself, which is useful to maintain an excitation in a single neuron
        \item{}
        an inhibitory synapse from the other neuron, to create mutual exclusive excitation
    \end{itemize}
    This simple memory cell should now be able to store information over a long time horizon, the input vectors are provided in the right way.
    Recent results have shown that this architecture is not capable of repeatedly storing information and I am not sure in what range the input values shall lie.
    As described in \cite{LTCNetworks}, the synaptic current is computed by multiplying a non-linear conductance with a potential difference.
    The potential difference is just a parameter $E_{ij}$ minus the potential of the postsynaptic neuron $V_j$.
    However, if the potentials are not bounded, this would mean even an excitatory synapse can deliver a negative current or analogously an inhibitory synapse can deliver a positive current.
    The bounded dynamics of LTC networks is no longer valid, as the leakage term was removed from the state dynamics equation.

    \chapter{Benchmarks}

    \section{Benchmark Framework}

    \section{Activity Benchmark}

    \section{Add Benchmark}

    \section{Walker Benchmark}
    This benchmark evaluates how well a model can predict a dynamical, physical system's behavior and was taken from \cite{LongTermDependenciesIrregularTimeSeries}.
    The training data is acquired simulation data of the \texttt{Walker2d-v2} OpenAI gym \cite{OpenAIGym} controlled by a pre-trained policy.
    The objective was to learn the kinematic simulation of the physics engine in an auto-regressive fashion using imitation learning.
    To increase the task difficulty, the simulation data was acquired from different training stages of the pre-trained policy (between 500 and 1200 Proximal Policy Optimization iterations) and 1\% of actions were overwritten by random actions.
    Furthermore, frame-skips were introduced by removing 10\% of all time-steps to get irregularly sampled training data.
    Moreover, the training data was divided into equally long sequences.
    For non-recurrent models, the input per time-step was the whole training sequence, but future values were zero-padded to ensure the model is only able to derive future predictions from past values.
    The model needs to predict the simulation state in the next time-step.
    Mean squared error was used as loss function when training the models.
    Further results of recurrent neural network models doing this benchmark can be found in \cite{LatentODEsIrregularlySampled}.
    The results are presented in the following table:

    \section{Memory Benchmark}
    This benchmark evaluates how well a model can be trained to incorporate long-term dependencies in its predictions.
    The structure of this benchmark was taken from \cite{UnitaryRNNs}, who also tested their recurrent neural network with this long-term dependency benchmark.
    The training data only contains integers from $0$ to $N-1$. The first ten input symbols of each input sequence are randomly sampled integers from $0$ to $N-3$ (i.i.d. uniform).
    Then the following symbols are copies of the integer $N-2$ in the amount of the tested memory length $T$ minus $1$.
    After that follows a marker, which is a single integer $N-1$.
    Then the remaining ten symbols are again copies of the integer $N-2$.
    The expected output sequence starts with $T+10$ copies of $N-2$ followed by the ten randomly sampled symbols from the start of the input sequence.
    Each tested model must produce an output vector of size $N$ per time-step to a apply a sparse categorical cross-entropy loss directly from the model output logits.
    The loss was weighted such that the loss given by each of the last ten output vectors has the same weight as all previous output vectors combined, as it is easily to optimize to predict only the same class for a long interval.
    Clearly, a model can only solve this task if it is able to store information over a time period in the same size of the memory length.
    The baseline for this benchmark's mean categorical entropy loss per batch can be set to $\frac{10\log(N-2)}{T+20}$, as a memory-less strategy can perfectly predict the first $T+10$ symbols of the required output sequence and then it predicts the remaining $10$ symbols randomly from $0$ to $N-3$ (i.i.d. uniform).
    The results are presented in the following table, where $N$ was $10$, $T$ was $100$ and the baseline therefore was $0.173$:

    \section{MNIST Benchmark}

    \section{Cell Benchmark}
    This is a very small benchmark that tries to find the right parameter and input ranges for the memory cell \ref{Memory Cell}.
    This benchmark should make clear if a memory cell is able to switch its state multiple times and keep information over a long period of time.
    It is directly implemented in the same file as the memory cell itself.
    The memory cell in the current test setup has no more recurrent, excitatory synapses.
    As discussed in our meeting the state of a memory cell is now only changed through four synapses, two per neuron.
    One synapse per neuron delivers the input via a synaptic activation and the other synapse ensures the opposite potentials of both neurons.
    The synaptic current can be computed by $g_{syn}(V_{pre}) * (E_{pre,post} - V_{post})$.
    I have found out that the LTC equations are not suited at all to build a memory cell, as a $1$ input should deliver a big current and a $0$ input should lead to a big negative current, no matter what the potential of the postsynaptic neuron is.
    However with the current equations, this is not possible to achieve, since the non-linear synaptic conductance is always positive and therefore the sign of the current cannot be determined by the input.
    Therefore, the underlying equations must be changed.
    The same argument holds for the mutual inhibitory synapses. The higher potential neuron should send a negative current to the other neuron, whereas the lower potential neuron should send a positive current to the higher potential neuron.
    These things are not possible with the current setup, and therefore need to be changed.
    The memory cell receives only a single input that will be passed untouched to the first neuron's synaptic activation, but the second neuron will get the negated input to its synaptic activation for the input.
    The negation is currently implemented using the function $f(x) = 1-x$.
    This input is only passed at the specific time step, when the input current provider needs to save something, otherwise both neurons in the memory cell get no input and should therefore hold their state.
    All memory cells are initialized to all contain zeros.
    As the output of the memory cell is the potential of the first neuron, this means that all first neurons in the memory cells have starting potential $0$ and all second neurons have starting potential $1$.
    The benchmark now feeds sparse inputs as described before to the memory cell, and requires the cell to hold the state from the last non-sparse input.
    This time horizon is the memory length of the memory cell, the amount of time it can save information.
    The benchmark not only checks the potential of the first neuron, but also the potential of the second neuron to be in the required range.
    Furthermore, the benchmarks alternates the symbol that should be saved first, half of the time it is a $0$, half of the time it is a $1$.
    Moreover, the memory cell state is switched again two times after the first input by providing two additional inputs, which represent the negated current memory cell potential, to check if the cell is able to switch its state.
    This results can be easily viewed when executing the memory cell Python script, they are not interesting since the memory cell does not learn anything.

    \chapter{Results}

    \chapter{Summary}
%\todo{Enter your text here.}

% Remove following line for the final thesis.
% \input{intro.tex} % A short introduction to LaTeX.

    \backmatter

% Use an optional list of figures.
    \listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
    \cleardoublepage % Start list of tables on the next empty right hand page.
    \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of algorithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
    \printindex

% Add a glossary.
    \printglossaries

% Add a bibliography.
    \bibliographystyle{alpha}
    \bibliography{thesis}

\end{document}