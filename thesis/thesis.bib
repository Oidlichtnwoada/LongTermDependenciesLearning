@misc{LatentODEsIrregularlySampled,
      title={Latent ODEs for Irregularly-Sampled Time Series}, 
      author={Yulia Rubanova and Ricky T. Q. Chen and David Duvenaud},
      year={2019},
      eprint={1907.03907},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{LongTermDependenciesIrregularTimeSeries,
      title={Learning Long-Term Dependencies in Irregularly-Sampled Time Series}, 
      author={Mathias Lechner and Ramin Hasani},
      year={2020},
      eprint={2006.04418},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{LSTM,
      issn = {1530-888X},
      abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is
      O
      . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
      journal = {Neural computation},
      pages = {1735--1780},
      volume = {9},
      publisher = {MIT Press - Journals},
      number = {8},
      year = {1997},
      title = {Long Short-Term Memory},
      copyright = {1997 Massachusetts Institute of Technology},
      language = {eng},
      address = {United States},
      author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
      keywords = {Learning ; Algorithms ; Time Factors ; Memory ; Models, Psychological ; Models, Neurological ; Memory, Short-Term ; Nerve Net - physiology ; Neural Networks (Computer)},
}
@ARTICLE{LongTermDependenciesGradientDescent,
      author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},
      journal={IEEE Transactions on Neural Networks}, 
      title={Learning long-term dependencies with gradient descent is difficult}, 
      year={1994},
      volume={5},
      number={2},
      pages={157-166},
}
@inbook{GradientDescent,
      author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
      title = {Learning Internal Representations by Error Propagation},
      year = {1986},
      isbn = {026268053X},
      publisher = {MIT Press},
      address = {Cambridge, MA, USA},
      booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
      pages = {318–362},
      numpages = {45}
}
@article{BPTT,
      author = {Werbos, Paul},
      year = {1990},
      month = {11},
      pages = {1550 - 1560},
      title = {Backpropagation through time: what it does and how to do it},
      volume = {78},
      journal = {Proceedings of the IEEE},
      doi = {10.1109/5.58337}
}
@misc{NeuralODEs,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{LTCNetworks,
      title={Liquid Time-constant Networks}, 
      author={Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
      year={2020},
      eprint={2006.04439},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Transformer,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Reformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{TransformerforRL,
      title={Stabilizing Transformers for Reinforcement Learning}, 
      author={Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},
      year={2019},
      eprint={1910.06764},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{NCP,
      author = {Lechner, Mathias and Hasani, Ramin and Amini, Alexander and Henzinger, Thomas and Rus, Daniela and Grosu, Radu},
      year = {2020},
      month = {10},
      pages = {642-652},
      title = {Neural circuit policies enabling auditable autonomy},
      volume = {2},
      journal = {Nature Machine Intelligence},
      doi = {10.1038/s42256-020-00237-3}
}
@misc{UnitaryRNNs,
      title={Unitary Evolution Recurrent Neural Networks}, 
      author={Martin Arjovsky and Amar Shah and Yoshua Bengio},
      year={2016},
      eprint={1511.06464},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{EfficientUnitaryRNNs,
      title={Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs}, 
      author={Li Jing and Yichen Shen and Tena Dubček and John Peurifoy and Scott Skirlo and Yann LeCun and Max Tegmark and Marin Soljačić},
      year={2017},
      eprint={1612.05231},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{RCNN_ReciprocalGatedCell,
      title={Task-Driven Convolutional Recurrent Models of the Visual System}, 
      author={Aran Nayebi and Daniel Bear and Jonas Kubilius and Kohitij Kar and Surya Ganguli and David Sussillo and James J. DiCarlo and Daniel L. K. Yamins},
      year={2018},
      eprint={1807.00053},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}
@misc{RRCNN,
      title={Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation}, 
      author={Md Zahangir Alom and Mahmudul Hasan and Chris Yakopcic and Tarek M. Taha and Vijayan K. Asari},
      year={2018},
      eprint={1802.06955},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{LinearTransformer,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{GPT-3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{TransformerBenchmark,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{GRU,
      title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
      author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1412.3555},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{WormInspiredControl,
  author={M. {Lechner} and R. {Hasani} and M. {Zimmer} and T. A. {Henzinger} and R. {Grosu}},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={Designing Worm-inspired Neural Networks for Interpretable Robotic Control}, 
  year={2019},
  volume={},
  number={},
  pages={87-94},
  doi={10.1109/ICRA.2019.8793840}
}
